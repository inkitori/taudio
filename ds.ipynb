{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c00609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/ark/anjo0/miniconda3/envs/taudio/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from dataset import get_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3d02694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2_5OmniProcessor, Qwen2_5OmniThinkerForConditionalGeneration\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-Omni-3B\"\n",
    "\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "def build_conversation(model_id, word):\n",
    "\tprocessor = Qwen2_5OmniProcessor.from_pretrained(model_id)\n",
    "\n",
    "\tconversation = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"system\",\n",
    "\t\t\t\"content\": [\n",
    "\t\t\t\t{\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n",
    "\t\t\t],\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": [\n",
    "\t\t\t\t{\"type\": \"audio\", \"audio\": \"PLACEHOLDER AUDIO\"}, # we will manually fill in the audio\n",
    "\t\t\t\t{\"type\": \"text\", \"text\": f\"When is \\\"{word}\\\" said?\"},\n",
    "\t\t\t],\n",
    "\t\t},\n",
    "\t]\n",
    "\n",
    "\ttext = processor.apply_chat_template(\n",
    "\t\tconversation,\n",
    "\t\ttokenize=False,\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t)\n",
    "\t\n",
    "\n",
    "\treturn text\n",
    "\n",
    "def get_ds(model_id, split='train_clean_100', slice=None):\n",
    "\tdef preprocess_fn(example):\n",
    "\t\taudio = example['audio']\n",
    "\t\twords = example['words']\n",
    "\n",
    "\t\tprompt = build_conversation(model_id, words[0]['word'])\n",
    "\t\taudio_frames = audio['array']\n",
    "\n",
    "\t\tinputs = processor(\n",
    "\t\t\ttext=prompt,\n",
    "\t\t\taudio=audio_frames,\n",
    "\t\t\treturn_tensors='pt',\n",
    "\t\t\tpadding=True,\n",
    "\t\t)\n",
    "\n",
    "\t\tinput_ids = inputs['input_ids']\n",
    "\t\tattention_mask = inputs['attention_mask']\n",
    "\t\tinput_features = inputs['input_features']\n",
    "\t\tfeature_attention_mask = inputs['feature_attention_mask']\n",
    "\n",
    "\t\ttime = feature_attention_mask.sum(dim=-1) # length of audio in centiseconds\n",
    "\t\tlabels = torch.zeros(int(time // 4)) # each embedding is 4 centiseconds long\n",
    "\t\tend_idx = int(words[0]['end'] * 25) # convert to centiseconds and divide by 4\n",
    "\t\t# TODO: clamp to max size of labels\n",
    "\n",
    "\t\tlabels[end_idx] = 1\n",
    "\n",
    "\t\t# audio_features = model.get_audio_features(\n",
    "\t\t# \tinput_features=input_features.to(device=model.device),\n",
    "\t\t# \tfeature_attention_mask=feature_attention_mask.to(device=model.device),\n",
    "\t\t# )\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t'prompt': prompt,\n",
    "\t\t\t'audio_frames': audio_frames,\n",
    "\t\t\t'input_ids': input_ids[0],\n",
    "\t\t\t'attention_mask': attention_mask[0],\n",
    "\t\t\t'input_features': input_features[0],\n",
    "\t\t\t'feature_attention_mask': feature_attention_mask[0],\n",
    "\t\t\t'labels': labels,\n",
    "\t\t\t# 'audio_features': audio_features.to('cpu'),\n",
    "\t\t}\n",
    "\n",
    "\tprocessor = Qwen2_5OmniProcessor.from_pretrained(model_id)\n",
    "\n",
    "\tbase_ds = load_dataset(\"gilkeyio/librispeech-alignments\")[split].select(range(slice)) if slice else load_dataset(\"gilkeyio/librispeech-alignments\")[split]\n",
    "\n",
    "\tds = base_ds.map(preprocess_fn, remove_columns=base_ds.column_names)\n",
    "\t\n",
    "\tds.set_format(type='torch')\n",
    "\n",
    "\treturn ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f4b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   2%|▏         | 514/28538 [14:44<13:57:59,  1.79s/ examples]"
     ]
    }
   ],
   "source": [
    "ds = get_ds('Qwen/Qwen2.5-Omni-3B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5083fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(861)\n",
      "torch.Size([215, 2048])\n",
      "torch.Size([215])\n"
     ]
    }
   ],
   "source": [
    "i=4\n",
    "print(ds[i]['feature_attention_mask'].sum())\n",
    "print(ds[i]['audio_features'].shape)\n",
    "print(ds[i]['labels'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
