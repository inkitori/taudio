{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a3612b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Initialize the visualizer\\nvisualizer = AttentionVisualizer('path/to/your/attention_weights.txt')\\n\\n# Plot a specific attention head with custom tick interval\\nvisualizer.plot_attention_heatmap(row_idx=0, head_idx=0, tick_interval=50)\\n\\n# Use interactive visualization with custom tick interval\\nvisualizer.interactive_heatmap(tick_interval=50)\\n\\n# Compare multiple heads with custom tick interval\\nvisualizer.compare_heads(row_idx=0, heads_to_compare=[0, 1, 2], tick_interval=50)\\n\\n# Get statistics\\nstats = visualizer.get_attention_stats(row_idx=0, head_idx=0)\\nprint(stats)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, IntSlider, Layout\n",
    "import ast\n",
    "\n",
    "class AttentionVisualizer:\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize the attention visualizer with the file path.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the attention weights file\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.attention_data = self._load_attention_data()\n",
    "        self.num_rows = len(self.attention_data)\n",
    "        \n",
    "        # Get dimensions from first row\n",
    "        first_tensor = self.attention_data[0]\n",
    "        self.num_heads = first_tensor.shape[1]\n",
    "        self.seq_len = first_tensor.shape[2]\n",
    "        \n",
    "        print(f\"Loaded attention data:\")\n",
    "        print(f\"  - Number of rows: {self.num_rows}\")\n",
    "        print(f\"  - Number of attention heads: {self.num_heads}\")\n",
    "        print(f\"  - Sequence length: {self.seq_len}\")\n",
    "    \n",
    "    def _load_attention_data(self):\n",
    "        \"\"\"Load and parse the attention data from the file.\"\"\"\n",
    "        attention_tensors = []\n",
    "        \n",
    "        with open(self.file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    # Parse the line as a Python literal (list)\n",
    "                    tensor_data = ast.literal_eval(line)\n",
    "                    # Convert to numpy array and reshape\n",
    "                    tensor = np.array(tensor_data)\n",
    "                    attention_tensors.append(tensor)\n",
    "        \n",
    "        return attention_tensors\n",
    "    \n",
    "    def _get_tick_positions(self, seq_len, tick_interval=50):\n",
    "        \"\"\"\n",
    "        Generate tick positions at specified intervals.\n",
    "        \n",
    "        Args:\n",
    "            seq_len (int): Length of the sequence\n",
    "            tick_interval (int): Interval between ticks\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (tick_positions, tick_labels)\n",
    "        \"\"\"\n",
    "        tick_positions = list(range(0, seq_len, tick_interval))\n",
    "        # Always include the last position if it's not already included\n",
    "        if tick_positions[-1] != seq_len - 1:\n",
    "            tick_positions.append(seq_len - 1)\n",
    "        \n",
    "        tick_labels = [str(pos) for pos in tick_positions]\n",
    "        return tick_positions, tick_labels\n",
    "    \n",
    "    def plot_attention_heatmap(self, row_idx=0, head_idx=0, figsize=(10, 8), cmap='Blues', tick_interval=50):\n",
    "        \"\"\"\n",
    "        Plot attention heatmap for a specific row and head.\n",
    "        \n",
    "        Args:\n",
    "            row_idx (int): Row index to visualize\n",
    "            head_idx (int): Attention head index to visualize\n",
    "            figsize (tuple): Figure size for the plot\n",
    "            cmap (str): Colormap for the heatmap\n",
    "            tick_interval (int): Interval between axis ticks (default: 50)\n",
    "        \"\"\"\n",
    "        if row_idx >= self.num_rows:\n",
    "            print(f\"Row index {row_idx} out of range. Max row index: {self.num_rows - 1}\")\n",
    "            return\n",
    "        \n",
    "        if head_idx >= self.num_heads:\n",
    "            print(f\"Head index {head_idx} out of range. Max head index: {self.num_heads - 1}\")\n",
    "            return\n",
    "        \n",
    "        # Extract the attention matrix for the specified row and head\n",
    "        attention_matrix = self.attention_data[row_idx][0, head_idx, :, :]\n",
    "        \n",
    "        # Get tick positions and labels\n",
    "        tick_positions, tick_labels = self._get_tick_positions(self.seq_len, tick_interval)\n",
    "        \n",
    "        # Create the heatmap\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(attention_matrix, \n",
    "                   cmap=cmap, \n",
    "                   cbar=True,\n",
    "                   square=True,\n",
    "                   xticklabels=tick_positions,\n",
    "                   yticklabels=tick_positions,\n",
    "                   cbar_kws={'label': 'Attention Weight'})\n",
    "        \n",
    "        # Set custom tick positions and labels\n",
    "        plt.xticks(tick_positions, tick_labels)\n",
    "        plt.yticks(tick_positions, tick_labels)\n",
    "        \n",
    "        plt.title(f'Attention Heatmap - Row {row_idx}, Head {head_idx}')\n",
    "        plt.xlabel('Key Position')\n",
    "        plt.ylabel('Query Position')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def interactive_heatmap(self, figsize=(12, 10), cmap='gray', tick_interval=50):\n",
    "        \"\"\"\n",
    "        Create an interactive heatmap with sliders for row and head selection.\n",
    "        \n",
    "        Args:\n",
    "            figsize (tuple): Figure size for the plot\n",
    "            cmap (str): Colormap for the heatmap\n",
    "            tick_interval (int): Interval between axis ticks (default: 50)\n",
    "        \"\"\"\n",
    "        def plot_heatmap(row_idx, head_idx):\n",
    "            self.plot_attention_heatmap(row_idx, head_idx, figsize, cmap, tick_interval)\n",
    "        \n",
    "        # Create sliders\n",
    "        row_slider = IntSlider(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=self.num_rows - 1,\n",
    "            step=1,\n",
    "            description='Row:',\n",
    "            layout=Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        head_slider = IntSlider(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=self.num_heads - 1,\n",
    "            step=1,\n",
    "            description='Head:',\n",
    "            layout=Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # Create interactive widget\n",
    "        interact(plot_heatmap, row_idx=row_slider, head_idx=head_slider)\n",
    "    \n",
    "    def compare_heads(self, row_idx=0, heads_to_compare=None, figsize=(15, 5), tick_interval=50):\n",
    "        \"\"\"\n",
    "        Compare multiple attention heads side by side for a given row.\n",
    "        \n",
    "        Args:\n",
    "            row_idx (int): Row index to visualize\n",
    "            heads_to_compare (list): List of head indices to compare. If None, shows first 3 heads.\n",
    "            figsize (tuple): Figure size for the plot\n",
    "            tick_interval (int): Interval between axis ticks (default: 50)\n",
    "        \"\"\"\n",
    "        if heads_to_compare is None:\n",
    "            heads_to_compare = list(range(min(3, self.num_heads)))\n",
    "        \n",
    "        if row_idx >= self.num_rows:\n",
    "            print(f\"Row index {row_idx} out of range. Max row index: {self.num_rows - 1}\")\n",
    "            return\n",
    "        \n",
    "        # Get tick positions and labels\n",
    "        tick_positions, tick_labels = self._get_tick_positions(self.seq_len, tick_interval)\n",
    "        \n",
    "        num_heads_to_show = len(heads_to_compare)\n",
    "        fig, axes = plt.subplots(1, num_heads_to_show, figsize=figsize)\n",
    "        \n",
    "        if num_heads_to_show == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, head_idx in enumerate(heads_to_compare):\n",
    "            if head_idx >= self.num_heads:\n",
    "                print(f\"Head index {head_idx} out of range. Max head index: {self.num_heads - 1}\")\n",
    "                continue\n",
    "            \n",
    "            attention_matrix = self.attention_data[row_idx][0, head_idx, :, :]\n",
    "            \n",
    "            sns.heatmap(attention_matrix, \n",
    "                       ax=axes[i],\n",
    "                       cmap='gray',\n",
    "                       cbar=True,\n",
    "                       square=True,\n",
    "                       xticklabels=tick_positions,\n",
    "                       yticklabels=tick_positions,\n",
    "                       cbar_kws={'label': 'Attention Weight'})\n",
    "            \n",
    "            # Set custom tick positions and labels\n",
    "            axes[i].set_xticks(tick_positions)\n",
    "            axes[i].set_xticklabels(tick_labels)\n",
    "            axes[i].set_yticks(tick_positions)\n",
    "            axes[i].set_yticklabels(tick_labels)\n",
    "            \n",
    "            axes[i].set_title(f'Head {head_idx}')\n",
    "            axes[i].set_xlabel('Key Position')\n",
    "            if i == 0:\n",
    "                axes[i].set_ylabel('Query Position')\n",
    "        \n",
    "        plt.suptitle(f'Attention Heads Comparison - Row {row_idx}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_attention_stats(self, row_idx=0, head_idx=0):\n",
    "        \"\"\"\n",
    "        Get statistics for a specific attention matrix.\n",
    "        \n",
    "        Args:\n",
    "            row_idx (int): Row index\n",
    "            head_idx (int): Head index\n",
    "            \n",
    "        Returns:\n",
    "            dict: Statistics about the attention matrix\n",
    "        \"\"\"\n",
    "        if row_idx >= self.num_rows or head_idx >= self.num_heads:\n",
    "            return None\n",
    "        \n",
    "        attention_matrix = self.attention_data[row_idx][0, head_idx, :, :]\n",
    "        \n",
    "        stats = {\n",
    "            'mean': np.mean(attention_matrix),\n",
    "            'std': np.std(attention_matrix),\n",
    "            'min': np.min(attention_matrix),\n",
    "            'max': np.max(attention_matrix),\n",
    "            'sparsity': np.sum(attention_matrix < 0.01) / attention_matrix.size,\n",
    "            'entropy': -np.sum(attention_matrix * np.log(attention_matrix + 1e-10), axis=-1).mean()\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Usage example:\n",
    "\"\"\"\n",
    "# Initialize the visualizer\n",
    "visualizer = AttentionVisualizer('path/to/your/attention_weights.txt')\n",
    "\n",
    "# Plot a specific attention head with custom tick interval\n",
    "visualizer.plot_attention_heatmap(row_idx=0, head_idx=0, tick_interval=50)\n",
    "\n",
    "# Use interactive visualization with custom tick interval\n",
    "visualizer.interactive_heatmap(tick_interval=50)\n",
    "\n",
    "# Compare multiple heads with custom tick interval\n",
    "visualizer.compare_heads(row_idx=0, heads_to_compare=[0, 1, 2], tick_interval=50)\n",
    "\n",
    "# Get statistics\n",
    "stats = visualizer.get_attention_stats(row_idx=0, head_idx=0)\n",
    "print(stats)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecf0ec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded attention data:\n",
      "  - Number of rows: 1\n",
      "  - Number of attention heads: 16\n",
      "  - Sequence length: 388\n"
     ]
    }
   ],
   "source": [
    "visualizer = AttentionVisualizer('attentions.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681646cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bc766d72e4413d9ec3e5157da84c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Row:', layout=Layout(width='400px'), max=0), IntSlider(vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualizer.interactive_heatmap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
