{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b0719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/ark/anjo0/miniconda3/envs/taudio/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "from transformers import Qwen2_5OmniThinkerForConditionalGeneration, Qwen2_5OmniProcessor\n",
    "from qwen_omni_utils import process_mm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b809c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.69s/it]\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2.5-Omni-3B\"\n",
    "model = Qwen2_5OmniThinkerForConditionalGeneration.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2850eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"gilkeyio/librispeech-alignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24d92a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 0.66, 'start': 0.5, 'word': 'the'}, {'end': 0.95, 'start': 0.66, 'word': 'dull'}, {'end': 1.41, 'start': 0.95, 'word': 'light'}, {'end': 1.71, 'start': 1.41, 'word': 'fell'}, {'end': 1.94, 'start': 1.71, 'word': 'more'}, {'end': 2.46, 'start': 1.94, 'word': 'faintly'}, {'end': 2.76, 'start': 2.46, 'word': 'upon'}, {'end': 2.84, 'start': 2.76, 'word': 'the'}, {'end': 3.4, 'start': 2.84, 'word': 'page'}, {'end': 3.81, 'start': 3.4, 'word': 'whereon'}, {'end': 4.22, 'start': 3.81, 'word': 'another'}, {'end': 4.85, 'start': 4.22, 'word': 'equation'}, {'end': 5.29, 'start': 4.85, 'word': 'began'}, {'end': 5.48, 'start': 5.29, 'word': 'to'}, {'end': 6.01, 'start': 5.48, 'word': 'unfold'}, {'end': 6.51, 'start': 6.01, 'word': 'itself'}, {'end': 7.1, 'start': 6.51, 'word': 'slowly'}, {'end': 7.72, 'start': 7.53, 'word': 'and'}, {'end': 7.82, 'start': 7.72, 'word': 'to'}, {'end': 8.24, 'start': 7.82, 'word': 'spread'}, {'end': 8.79, 'start': 8.24, 'word': 'abroad'}, {'end': 8.97, 'start': 8.79, 'word': 'its'}, {'end': 9.56, 'start': 8.97, 'word': 'widening'}, {'end': 10.09, 'start': 9.59, 'word': 'tail'}]\n"
     ]
    }
   ],
   "source": [
    "print(ds['test_clean']['words'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    # {\n",
    "    #     \"role\": \"system\",\n",
    "    #     \"content\": [\n",
    "    #         {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n",
    "    #     ],\n",
    "    # },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": ds['test_clean']['audio'][0]},\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7960c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\n",
      "/gscratch/ark/anjo0/miniconda3/envs/taudio/lib/python3.12/site-packages/librosa/core/audio.py:172: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m inputs = inputs.to(model.device).to(model.dtype)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Inference: Generation of the output text and audio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m text_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\u001b[32m     16\u001b[39m text = processor.batch_decode(text_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# set use audio in video\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "# audios = ds['test_clean']['audio'][0]['array']\n",
    "# images = None\n",
    "# videos = None\n",
    "inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "# Inference: Generation of the output text and audio\n",
    "text_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "\n",
    "text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(text)\n",
    "sf.write(\n",
    "    \"output.wav\",\n",
    "    audio.reshape(-1).detach().cpu().numpy(),\n",
    "    samplerate=24000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0c131b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.32s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:8292 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\nRepeat exactly what is said in this audio\\nassistant\\nThe dull light fell more faintly upon the page whereon another equation began to unfold itself slowly and to spread abroad its widening tale. If you have any other parts of the audio you want me to repeat or any other questions, feel free to let me know.']\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-Omni-3B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-Omni-3B\",\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\",\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "# )\n",
    "\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-3B\")\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n",
    "            {\"type\": \"text\", \"text\": \"Repeat exactly what is said in this audio\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# set use audio in video\n",
    "USE_AUDIO_IN_VIDEO = True\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "# audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "audios = list(ds['test_clean']['audio'][0]['array'])\n",
    "images = None\n",
    "videos = None\n",
    "inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "# Inference: Generation of the output text and audio\n",
    "text_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "\n",
    "text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(text)\n",
    "sf.write(\n",
    "    \"output.wav\",\n",
    "    audio.reshape(-1).detach().cpu().numpy(),\n",
    "    samplerate=24000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eaa28ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '1089-134686-0006.flac', 'array': array([-0.00057983, -0.0007019 , -0.0005188 , ..., -0.00067139,\n",
      "       -0.00027466, -0.00054932]), 'sampling_rate': 16000}\n"
     ]
    }
   ],
   "source": [
    "print(ds['test_clean']['audio'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
